#!/usr/bin/env python3
"""
s3_bucket_sizes_to_md.py

When executed, this script:
- Discovers the AWS identity boto3 is using (sts:GetCallerIdentity)
- Lists all S3 buckets in the account
- Computes total size per bucket by summing object sizes (optionally under a prefix)
- Writes a README-style Markdown report to: s3bucket.md

Usage examples:
  python3 s3_bucket_sizes_to_md.py
  python3 s3_bucket_sizes_to_md.py --profile myprofile
  python3 s3_bucket_sizes_to_md.py --output s3bucket.md
  python3 s3_bucket_sizes_to_md.py --max-workers 16
  python3 s3_bucket_sizes_to_md.py --prefix logs/
"""

from __future__ import annotations

import argparse
import concurrent.futures as cf
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError


# ------------------------- formatting helpers -------------------------

def bytes_to_units(num_bytes: int) -> Dict[str, float]:
    # Using 1024-based units (MiB/GiB/TiB) but labeling as MB/GB/TB as requested.
    mb = num_bytes / (1024 ** 2)
    gb = num_bytes / (1024 ** 3)
    tb = num_bytes / (1024 ** 4)
    return {"MB": mb, "GB": gb, "TB": tb}


def fmt_units(num_bytes: int) -> str:
    u = bytes_to_units(num_bytes)
    return f"{u['MB']:,.2f} MB | {u['GB']:,.4f} GB | {u['TB']:,.6f} TB"


def md_escape(text: str) -> str:
    # Minimal escaping for Markdown table cells
    return text.replace("|", "\\|")


# ------------------------- S3 sizing -------------------------

@dataclass(frozen=True)
class BucketSizeResult:
    bucket: str
    bytes_total: int
    objects_count: int
    error: Optional[str] = None


def iter_object_sizes(
    s3_client,
    bucket: str,
    prefix: Optional[str] = None,
) -> Iterable[Tuple[int, int]]:
    paginator = s3_client.get_paginator("list_objects_v2")
    kwargs = {"Bucket": bucket}
    if prefix:
        kwargs["Prefix"] = prefix

    for page in paginator.paginate(**kwargs):
        for obj in page.get("Contents", []) or []:
            yield int(obj.get("Size", 0)), 1


def compute_bucket_size(
    s3_client,
    bucket: str,
    prefix: Optional[str] = None,
) -> BucketSizeResult:
    try:
        total = 0
        count = 0
        for size_bytes, inc in iter_object_sizes(s3_client, bucket, prefix):
            total += size_bytes
            count += inc
        return BucketSizeResult(bucket=bucket, bytes_total=total, objects_count=count)
    except ClientError as e:
        return BucketSizeResult(bucket=bucket, bytes_total=0, objects_count=0, error=str(e))
    except Exception as e:
        return BucketSizeResult(bucket=bucket, bytes_total=0, objects_count=0, error=str(e))


# ------------------------- report generation -------------------------

def build_markdown_report(
    *,
    generated_at_utc: str,
    identity: Dict[str, str],
    profile: Optional[str],
    prefix: Optional[str],
    results: List[BucketSizeResult],
) -> str:
    ok = [r for r in results if not r.error]
    errs = [r for r in results if r.error]

    total_bytes = sum(r.bytes_total for r in ok)
    total_units = fmt_units(total_bytes)

    # Sort: biggest first, then errors at bottom
    results_sorted = sorted(results, key=lambda r: (r.error is not None, -r.bytes_total, r.bucket.lower()))

    lines: List[str] = []
    lines.append("# S3 Bucket Storage Report\n")
    lines.append("This file is auto-generated by `s3_bucket_sizes_to_md.py`.\n")
    lines.append("## Run Context\n")
    lines.append(f"- **Generated (UTC):** {generated_at_utc}")
    lines.append(f"- **AWS Profile:** {profile or '(default credential chain)'}")
    lines.append(f"- **Prefix Filter:** {prefix or '(none)'}\n")

    lines.append("## AWS Identity (boto3 / STS)\n")
    lines.append(f"- **Account:** `{identity.get('Account', '')}`")
    lines.append(f"- **Arn:** `{identity.get('Arn', '')}`")
    lines.append(f"- **UserId:** `{identity.get('UserId', '')}`\n")

    lines.append("## Account Summary\n")
    lines.append(f"- **Buckets discovered:** {len(results)}")
    lines.append(f"- **Buckets sized successfully:** {len(ok)}")
    lines.append(f"- **Buckets with errors:** {len(errs)}")
    lines.append(f"- **Total storage (successful buckets):** {total_units}")
    lines.append(f"- **Total bytes (successful buckets):** `{total_bytes:,d}`\n")

    lines.append("## Per-Bucket Details\n")
    lines.append("| Bucket | Objects | Bytes | MB / GB / TB | Status |")
    lines.append("|---|---:|---:|---:|---|")

    for r in results_sorted:
        status = "OK" if not r.error else f"ERROR: {md_escape(r.error)}"
        lines.append(
            f"| `{md_escape(r.bucket)}` "
            f"| {r.objects_count:,d} "
            f"| `{r.bytes_total:,d}` "
            f"| {fmt_units(r.bytes_total)} "
            f"| {status} |"
        )

    lines.append("\n## Notes\n")
    lines.append("- Sizes are computed by summing object sizes via `ListObjectsV2` pagination (read-only).")
    lines.append("- Very large buckets may take significant time to enumerate.")
    lines.append("- Report total excludes buckets that returned errors.\n")

    return "\n".join(lines)


# ------------------------- main -------------------------

def main() -> int:
    ap = argparse.ArgumentParser(description="Compute S3 bucket sizes and write a Markdown report to s3bucket.md.")
    ap.add_argument("--profile", default=None, help="AWS profile name (optional).")
    ap.add_argument("--prefix", default=None, help="Optional prefix filter (sizes only keys under this prefix).")
    ap.add_argument("--max-workers", type=int, default=8, help="Parallel workers across buckets (default: 8).")
    ap.add_argument("--no-parallel", action="store_true", help="Disable parallel sizing.")
    ap.add_argument("--output", default="s3bucket.md", help="Output markdown filename (default: s3bucket.md).")
    args = ap.parse_args()

    session = boto3.Session(profile_name=args.profile) if args.profile else boto3.Session()

    cfg = Config(retries={"max_attempts": 10, "mode": "standard"})
    s3 = session.client("s3", config=cfg)
    sts = session.client("sts", config=cfg)

    # Determine identity (the "account information used by boto3")
    identity = sts.get_caller_identity()  # contains Account, Arn, UserId

    # List all buckets
    buckets_resp = s3.list_buckets()
    buckets = [b["Name"] for b in buckets_resp.get("Buckets", [])]

    # Size all buckets
    results: List[BucketSizeResult] = []
    if not buckets:
        results = []
    elif args.no_parallel or args.max_workers <= 1:
        for name in buckets:
            results.append(compute_bucket_size(s3, name, prefix=args.prefix))
    else:
        with cf.ThreadPoolExecutor(max_workers=args.max_workers) as ex:
            futs = [ex.submit(compute_bucket_size, s3, name, args.prefix) for name in buckets]
            for fut in cf.as_completed(futs):
                results.append(fut.result())

    generated_at_utc = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S %Z")

    md = build_markdown_report(
        generated_at_utc=generated_at_utc,
        identity={"Account": identity.get("Account", ""), "Arn": identity.get("Arn", ""), "UserId": identity.get("UserId", "")},
        profile=args.profile,
        prefix=args.prefix,
        results=results,
    )

    out_path = Path(args.output).resolve()
    out_path.write_text(md, encoding="utf-8")

    print(f"Wrote Markdown report to: {out_path}")
    print(f"AWS Account: {identity.get('Account')}  Arn: {identity.get('Arn')}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
